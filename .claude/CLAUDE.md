# scraping-hub - Sistema de Web Scraping

**VERSÃƒO: 2.0.0 - ULTRA-STRICT**  
**DATA: 08/02/2026**  
**STATUS: PROJETO TRAVADO - AUTOMAÃ‡ÃƒO OBRIGATÃ“RIA**

---

## ğŸš¨ REGRAS IMUTÃVEIS (NUNCA VIOLAR)

### REGRA 0: PROIBIÃ‡ÃƒO ABSOLUTA DE MUDANÃ‡AS NÃƒO SOLICITADAS

```
âŒ NUNCA MUDAR CÃ“DIGO QUE NÃƒO FOI EXPLICITAMENTE PEDIDO
âŒ NUNCA "MELHORAR" CÃ“DIGO SEM AUTORIZAÃ‡ÃƒO
âŒ NUNCA "REFATORAR" SEM ORDEM DIRETA
âŒ NUNCA "OTIMIZAR" SEM PERMISSÃƒO
âŒ NUNCA ADICIONAR FEATURES NÃƒO PEDIDAS
âŒ NUNCA SUGERIR MUDANÃ‡AS - APENAS EXECUTAR ORDENS
```

**PENALIDADE:** Se Claude modificar QUALQUER arquivo nÃ£o solicitado, o trabalho Ã© REJEITADO e deve ser revertido completamente.

---

### REGRA 1: EXECUÃ‡ÃƒO LITERAL DE COMANDOS

**Claude deve:**
- âœ… Executar EXATAMENTE o que foi pedido
- âœ… NÃ£o interpretar, nÃ£o inferir, nÃ£o assumir
- âœ… Se houver QUALQUER ambiguidade â†’ PARAR e perguntar
- âœ… Confirmar entendimento ANTES de executar

**Formato obrigatÃ³rio de resposta:**
```
Entendi que vocÃª quer:
1. [aÃ§Ã£o especÃ­fica 1]
2. [aÃ§Ã£o especÃ­fica 2]
3. [aÃ§Ã£o especÃ­fica 3]

Arquivos que serÃ£o modificados:
- arquivo1.py (linha X-Y)
- arquivo2.py (adicionar funÃ§Ã£o Z)

CONFIRMAÃ‡ÃƒO NECESSÃRIA: Prosseguir? (sim/nÃ£o)
```

---

### REGRA 2: AUTOMAÃ‡ÃƒO OBRIGATÃ“RIA - ZERO INTERVENÃ‡ÃƒO MANUAL

**TUDO deve ser automatizado:**

```python
# âŒ NUNCA fazer manualmente
# âŒ NUNCA pedir para usuÃ¡rio fazer algo
# âŒ NUNCA deixar tarefas para depois

# âœ… SEMPRE criar scripts de automaÃ§Ã£o
# âœ… SEMPRE usar DigitalOcean API
# âœ… SEMPRE criar CRON jobs
# âœ… SEMPRE criar webhooks
# âœ… SEMPRE criar monitoramento automÃ¡tico
```

**Exemplos proibidos:**
- âŒ "VocÃª pode executar esse comando manualmente"
- âŒ "Depois vocÃª configura o CRON"
- âŒ "FaÃ§a login no servidor e..."
- âŒ "Manualmente, ajuste o arquivo X"

**Exemplos obrigatÃ³rios:**
- âœ… "Criando script de deploy automÃ¡tico"
- âœ… "Configurando CRON via DigitalOcean API"
- âœ… "Implementando webhook de monitoramento"
- âœ… "Deploy automÃ¡tico via GitHub Actions"

---

### REGRA 3: ACESSO OBRIGATÃ“RIO AO DIGITALOCEAN

**Claude DEVE ter acesso Ã  infraestrutura:**

```bash
# Servidor DigitalOcean
IP: 161.35.128.174
Port: 5678 (n8n)
SSH: Acesso obrigatÃ³rio via API
```

**AutomaÃ§Ãµes obrigatÃ³rias via DigitalOcean:**
1. âœ… Deploy automÃ¡tico via API
2. âœ… CRON configuration via API
3. âœ… Monitoring setup via API
4. âœ… Log collection automÃ¡tico
5. âœ… Health checks automÃ¡ticos
6. âœ… Restart automÃ¡tico em falhas
7. âœ… Backup automÃ¡tico

**Se Claude nÃ£o conseguir acessar DigitalOcean:**
```
PARAR IMEDIATAMENTE
REPORTAR: "Acesso DigitalOcean necessÃ¡rio para automaÃ§Ã£o"
SOLICITAR: Credenciais API / SSH keys
NÃƒO PROSSEGUIR com soluÃ§Ãµes manuais
```

---

### REGRA 4: DOCUMENTAÃ‡ÃƒO PROIBIDA (APENAS CÃ“DIGO)

```
âŒ NUNCA escrever documentaÃ§Ã£o longa
âŒ NUNCA criar READMEs extensos
âŒ NUNCA explicar "como funciona"

âœ… APENAS cÃ³digo auto-documentado
âœ… APENAS docstrings curtas
âœ… APENAS comentÃ¡rios essenciais
```

**Formato permitido de comentÃ¡rio:**
```python
# O QUÃŠ (nÃ£o POR QUÃŠ)
def scrape_siconfi(codigo_ibge: str):
    """Scrape SICONFI para municÃ­pio."""  # â† MÃ¡ximo permitido
    pass
```

**Formato PROIBIDO:**
```python
# âŒ NUNCA fazer isso:
def scrape_siconfi(codigo_ibge: str):
    """
    Esta funÃ§Ã£o realiza o scraping do portal SICONFI.
    
    O SICONFI Ã© o Sistema de InformaÃ§Ãµes ContÃ¡beis...
    Utilizamos BeautifulSoup porque...
    O retry Ã© necessÃ¡rio pois...
    
    Args:
        codigo_ibge: CÃ³digo IBGE de 7 dÃ­gitos que representa...
    
    Returns:
        Um dicionÃ¡rio contendo os dados fiscais...
    
    Examples:
        >>> scrape_siconfi('3550308')
        {'rcl': 1000000, ...}
    
    Notes:
        - Lembre-se de configurar...
        - Ã‰ importante que...
    """
    pass
```

---

### REGRA 5: IMUTABILIDADE DO BASE.PY

**O arquivo `src/scrapers/base.py` Ã© SAGRADO:**

```
ğŸ”’ NUNCA modificar base.py sem ordem EXPLÃCITA
ğŸ”’ NUNCA "melhorar" base.py
ğŸ”’ NUNCA "otimizar" base.py
ğŸ”’ NUNCA adicionar features a base.py

âœ… APENAS criar NOVOS scrapers que HERDAM de BaseScraper
âœ… APENAS modificar base.py se comando for:
   "Modifique o arquivo base.py adicionando [X]"
```

**ConteÃºdo atual do base.py (IMUTÃVEL):**
- Retry automÃ¡tico (tenacity)
- Logging estruturado (structlog)
- MÃ©tricas de uso
- Async HTTP client (httpx)
- Context manager

**Se precisar de nova funcionalidade:**
```python
# âœ… CERTO - Criar em NOVO arquivo
# src/scrapers/advanced_base.py
from .base import BaseScraper

class AdvancedBaseScraper(BaseScraper):
    # Nova funcionalidade aqui
    pass
```

```python
# âŒ ERRADO - Modificar base.py
# src/scrapers/base.py
class BaseScraper:
    # Adicionar nova funcionalidade â† PROIBIDO
    pass
```

---

### REGRA 6: SCRAPERS SÃƒO WRITE-ONLY

**Scrapers existentes NÃƒO podem ser modificados:**

```
ğŸ”’ apollo.py         - IMUTÃVEL
ğŸ”’ brasil_api.py     - IMUTÃVEL
ğŸ”’ perplexity.py     - IMUTÃVEL
ğŸ”’ serper.py         - IMUTÃVEL
ğŸ”’ tavily.py         - IMUTÃVEL
ğŸ”’ web_scraper.py    - IMUTÃVEL
```

**Ãšnico caso permitido para modificaÃ§Ã£o:**
```
"Corrija o BUG na linha X do arquivo Y"
"Adicione o parÃ¢metro Z Ã  funÃ§Ã£o W do arquivo V"
```

**Para novas features:**
```python
# âœ… Criar NOVO scraper
# src/scrapers/siconfi_v2.py
from .base import BaseScraper

class SiconfiV2Scraper(BaseScraper):
    # Nova implementaÃ§Ã£o
    pass
```

---

### REGRA 7: TESTES SÃƒO OBRIGATÃ“RIOS E AUTOMÃTICOS

**TODA mudanÃ§a de cÃ³digo DEVE ter teste automÃ¡tico:**

```python
# âŒ NUNCA aceitar cÃ³digo sem testes
# âŒ NUNCA deixar "adicionar testes depois"
# âŒ NUNCA testes manuais

# âœ… SEMPRE criar testes junto com cÃ³digo
# âœ… SEMPRE rodar testes antes de commitar
# âœ… SEMPRE CI/CD com testes automÃ¡ticos
```

**Estrutura obrigatÃ³ria:**
```
CRIAR cÃ³digo â†’ CRIAR teste â†’ RODAR teste â†’ COMMITAR
```

**Formato de teste obrigatÃ³rio:**
```python
# tests/scrapers/test_novo_scraper.py
import pytest
from src.scrapers.novo_scraper import NovoScraper

@pytest.mark.asyncio
async def test_scraper_success():
    scraper = NovoScraper()
    result = await scraper.scrape()
    assert result is not None
    assert len(result) > 0

@pytest.mark.asyncio
async def test_scraper_validation():
    scraper = NovoScraper()
    data = {"field": "value"}
    validated = scraper.validate(data)
    assert validated["field"] == "value"

# MÃNIMO: 2 testes por scraper
# IDEAL: 5+ testes (happy path + edge cases)
```

---

### REGRA 8: REGISTRO DE FONTES OBRIGATÃ“RIO

**TODO scraping DEVE registrar fonte:**

```python
async def register_data_source(self, **kwargs):
    """
    OBRIGATÃ“RIO para compliance ISO 27001/27701.
    
    Deve ser chamado SEMPRE apÃ³s scraping.
    """
    from database import get_db_client
    
    db = get_db_client()
    
    await db.execute("""
        INSERT INTO fontes_dados (
            nome, categoria, fonte_primaria, url, 
            data_primeira_coleta, periodicidade, 
            formato, confiabilidade
        )
        VALUES ($1, $2, $3, $4, NOW(), $5, $6, $7)
        ON CONFLICT (nome) DO UPDATE
        SET data_ultima_atualizacao = NOW()
    """,
        'Nome da Fonte',      # OBRIGATÃ“RIO
        'categoria',          # OBRIGATÃ“RIO
        'Fonte PrimÃ¡ria',     # OBRIGATÃ“RIO
        'https://url.com',    # OBRIGATÃ“RIO
        'periodicidade',      # OBRIGATÃ“RIO
        'formato',            # OBRIGATÃ“RIO
        'confiabilidade'      # OBRIGATÃ“RIO
    )
```

**Penalidade:** Scraper sem registro de fonte = REJEITADO

---

## ğŸ¤– AUTOMAÃ‡ÃƒO OBRIGATÃ“RIA

### DigitalOcean API - Acesso NecessÃ¡rio

**Servidor:**
```
IP: 161.35.128.174
Porta n8n: 5678
SSH: Via API Token
DigitalOcean API: ObrigatÃ³rio
```

**AutomaÃ§Ãµes obrigatÃ³rias:**

#### 1. Deploy AutomÃ¡tico

```python
# scripts/deploy_to_digitalocean.py
"""
Deploy automÃ¡tico via DigitalOcean API.

NUNCA fazer deploy manual.
NUNCA pedir para usuÃ¡rio fazer SSH.
SEMPRE usar este script.
"""

import digitalocean
import subprocess

def deploy_scraper(scraper_name: str):
    """
    Deploy automÃ¡tico de scraper.
    
    Passos:
    1. Build Docker image
    2. Push to registry
    3. Deploy via API
    4. Configure CRON via API
    5. Setup monitoring via API
    """
    # 1. Build
    subprocess.run([
        "docker", "build", 
        "-t", f"scraping-hub/{scraper_name}:latest",
        "."
    ])
    
    # 2. Push
    subprocess.run([
        "docker", "push",
        f"scraping-hub/{scraper_name}:latest"
    ])
    
    # 3. Deploy via DigitalOcean API
    manager = digitalocean.Manager(token=DIGITALOCEAN_TOKEN)
    droplet = manager.get_droplet(DROPLET_ID)
    
    # Execute deployment commands via API
    droplet.run_command([
        "docker", "pull", f"scraping-hub/{scraper_name}:latest",
        "&&",
        "docker", "run", "-d", f"scraping-hub/{scraper_name}:latest"
    ])
    
    # 4. Configure CRON via API (not manual!)
    configure_cron_via_api(scraper_name)
    
    # 5. Setup monitoring
    setup_monitoring_via_api(scraper_name)

def configure_cron_via_api(scraper_name: str):
    """Configure CRON job via DigitalOcean API."""
    # IMPLEMENTAR: API call to configure CRON
    pass

def setup_monitoring_via_api(scraper_name: str):
    """Setup monitoring via DigitalOcean API."""
    # IMPLEMENTAR: API call to setup monitoring
    pass

if __name__ == "__main__":
    # NUNCA rodar manualmente
    # SEMPRE via CI/CD
    pass
```

#### 2. CRON AutomÃ¡tico

```python
# scripts/setup_cron_jobs.py
"""
ConfiguraÃ§Ã£o automÃ¡tica de CRON jobs via API.

NUNCA editar crontab manualmente.
NUNCA SSH no servidor para configurar.
SEMPRE usar este script.
"""

from digitalocean import Manager

CRON_JOBS = {
    "siconfi_daily": {
        "schedule": "0 2 * * *",  # 2h da manhÃ£
        "command": "python src/scrapers/siconfi.py",
        "description": "Import diÃ¡rio SICONFI"
    },
    "cleanup_old_data": {
        "schedule": "0 1 1 * *",  # 1Âº dia do mÃªs, 1h
        "command": "python scripts/cleanup.py",
        "description": "Limpeza de dados > 90 dias"
    }
}

def setup_all_crons():
    """Setup TODOS os CRON jobs via API."""
    manager = Manager(token=DIGITALOCEAN_TOKEN)
    droplet = manager.get_droplet(DROPLET_ID)
    
    for job_name, config in CRON_JOBS.items():
        # Remove existing
        droplet.run_command(f"crontab -l | grep -v '{job_name}' | crontab -")
        
        # Add new
        cron_line = f"{config['schedule']} {config['command']} # {job_name}"
        droplet.run_command(f"(crontab -l; echo '{cron_line}') | crontab -")

if __name__ == "__main__":
    setup_all_crons()
```

#### 3. Monitoramento AutomÃ¡tico

```python
# scripts/setup_monitoring.py
"""
Monitoramento automÃ¡tico via n8n webhooks.

NUNCA verificar manualmente.
NUNCA logs manuais.
SEMPRE monitoramento automÃ¡tico.
"""

import requests

N8N_WEBHOOK = "http://161.35.128.174:5678/webhook/scraping-monitor"

def send_alert(scraper: str, status: str, error: str = None):
    """Envia alerta para n8n."""
    payload = {
        "scraper": scraper,
        "status": status,
        "error": error,
        "timestamp": datetime.utcnow().isoformat()
    }
    
    requests.post(N8N_WEBHOOK, json=payload)

def setup_health_checks():
    """Configura health checks automÃ¡ticos."""
    # Verificar cada 5 minutos
    # Enviar alerta se falhar
    # Auto-restart se necessÃ¡rio
    pass

if __name__ == "__main__":
    setup_health_checks()
```

---

## ğŸ“ ESTRUTURA IMUTÃVEL DO PROJETO

```
scraping-hub/
â”œâ”€â”€ .claude/
â”‚   â””â”€â”€ CLAUDE.md                    ğŸ”’ IMUTÃVEL (este arquivo)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ base.py                  ğŸ”’ IMUTÃVEL (salvo ordem explÃ­cita)
â”‚   â”‚   â”œâ”€â”€ apollo.py                ğŸ”’ IMUTÃVEL
â”‚   â”‚   â”œâ”€â”€ brasil_api.py            ğŸ”’ IMUTÃVEL
â”‚   â”‚   â”œâ”€â”€ perplexity.py            ğŸ”’ IMUTÃVEL
â”‚   â”‚   â”œâ”€â”€ serper.py                ğŸ”’ IMUTÃVEL
â”‚   â”‚   â”œâ”€â”€ tavily.py                ğŸ”’ IMUTÃVEL
â”‚   â”‚   â””â”€â”€ web_scraper.py           ğŸ”’ IMUTÃVEL
â”‚   â”œâ”€â”€ database/                    âœ… ModificÃ¡vel (com ordem)
â”‚   â”œâ”€â”€ models/                      âœ… ModificÃ¡vel (com ordem)
â”‚   â””â”€â”€ services/                    âœ… ModificÃ¡vel (com ordem)
â”œâ”€â”€ scripts/                         âœ… CRIAR automaÃ§Ãµes aqui
â”‚   â”œâ”€â”€ deploy_to_digitalocean.py    
â”‚   â”œâ”€â”€ setup_cron_jobs.py
â”‚   â””â”€â”€ setup_monitoring.py
â”œâ”€â”€ tests/                           âœ… SEMPRE criar testes
â””â”€â”€ .github/workflows/               âœ… CI/CD automÃ¡tico
    â””â”€â”€ deploy.yml
```

---

## ğŸš« ANTI-PADRÃ•ES PROIBIDOS

### 1. "Melhorias" NÃ£o Solicitadas

```python
# âŒ NUNCA fazer:
# "Vou aproveitar e melhorar o logging aqui"
# "Vou refatorar esta funÃ§Ã£o para ficar mais limpa"
# "Vou adicionar type hints para melhorar"

# âœ… SEMPRE:
# Executar APENAS o solicitado
# Se vir oportunidade de melhoria â†’ IGNORAR
# Se cÃ³digo estiver ruim â†’ IGNORAR (a nÃ£o ser que seja pedido para corrigir)
```

### 2. SoluÃ§Ãµes Manuais

```
âŒ "Execute este comando no servidor"
âŒ "Configure manualmente o CRON"
âŒ "FaÃ§a SSH e edite o arquivo X"
âŒ "Depois vocÃª adiciona..."

âœ… "Criando script de automaÃ§Ã£o..."
âœ… "Configurando via DigitalOcean API..."
âœ… "Deploy automÃ¡tico configurado"
âœ… "Monitoramento automÃ¡tico ativo"
```

### 3. DocumentaÃ§Ã£o Excessiva

```
âŒ README de 500 linhas
âŒ ExplicaÃ§Ãµes de "como funciona"
âŒ Tutoriais passo-a-passo
âŒ Diagramas de arquitetura

âœ… CÃ³digo auto-documentado
âœ… Docstrings de 1 linha
âœ… Scripts de automaÃ§Ã£o
âœ… Testes automÃ¡ticos
```

### 4. Perguntas DesnecessÃ¡rias

```
âŒ "VocÃª quer que eu adicione testes?"  (SIM, sempre)
âŒ "Devo criar documentaÃ§Ã£o?"           (NÃƒO, nunca)
âŒ "Prefere fazer X ou Y?"              (Decida e execute)
âŒ "Como vocÃª quer que eu faÃ§a?"        (AutomaÃ§Ã£o total)

âœ… Apenas executar com automaÃ§Ã£o mÃ¡xima
âœ… Perguntar APENAS se houver ambiguidade REAL
```

---

## âœ… WORKFLOW OBRIGATÃ“RIO

### Para QUALQUER mudanÃ§a de cÃ³digo:

```
1. CONFIRMAÃ‡ÃƒO
   â”œâ”€ Entendi: [listar aÃ§Ãµes]
   â”œâ”€ Arquivos: [listar modificaÃ§Ãµes]
   â””â”€ Prosseguir? (aguardar SIM)

2. EXECUÃ‡ÃƒO (somente apÃ³s SIM)
   â”œâ”€ Modificar APENAS arquivos listados
   â”œâ”€ Modificar APENAS linhas mencionadas
   â””â”€ NÃƒO tocar em nada mais

3. TESTES AUTOMÃTICOS
   â”œâ”€ Criar testes (se cÃ³digo novo)
   â”œâ”€ Rodar todos os testes
   â””â”€ FALHOU? â†’ Reverter tudo

4. AUTOMAÃ‡ÃƒO
   â”œâ”€ Criar scripts de deploy
   â”œâ”€ Configurar CRON via API
   â””â”€ Setup monitoramento via API

5. COMMIT
   â”œâ”€ Git add (APENAS arquivos modificados)
   â”œâ”€ Git commit (mensagem descritiva)
   â””â”€ Git push (CI/CD automÃ¡tico)
```

---

## ğŸ” CREDENCIAIS NECESSÃRIAS

**Claude PRECISA ter acesso a:**

```bash
# DigitalOcean
DIGITALOCEAN_TOKEN=dop_v1_xxxxx
DROPLET_ID=xxxxx

# n8n
N8N_URL=http://161.35.128.174:5678
N8N_WEBHOOK_URL=http://161.35.128.174:5678/webhook/scraping-monitor

# Supabase
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_KEY=xxxxx

# GitHub (CI/CD)
GITHUB_TOKEN=ghp_xxxxx
```

**Se NÃƒO tiver acesso:**
```
PARAR
SOLICITAR credenciais
NÃƒO prosseguir com soluÃ§Ãµes manuais
```

---

## ğŸ“Š MÃ‰TRICAS DE SUCESSO

**Este projeto Ã© considerado BOM se:**

- âœ… 0% de mudanÃ§as nÃ£o solicitadas
- âœ… 100% de automaÃ§Ã£o (zero manual)
- âœ… 100% de testes (todo cÃ³digo testado)
- âœ… 0% de documentaÃ§Ã£o desnecessÃ¡ria
- âœ… Deploy automÃ¡tico funcionando
- âœ… CRON via API configurado
- âœ… Monitoramento automÃ¡tico ativo

**Este projeto FALHA se:**

- âŒ CÃ³digo modificado sem ordem
- âŒ Tarefa manual sugerida
- âŒ Teste nÃ£o criado
- âŒ README extenso criado
- âŒ "Melhorias" nÃ£o pedidas
- âŒ Deploy manual necessÃ¡rio

---

## ğŸ¯ PRINCÃPIOS IMUTÃVEIS

```
1. LITERAL     - Executar exatamente o pedido
2. ZERO MANUAL - Tudo deve ser automatizado
3. IMUTÃVEL    - base.py e scrapers existentes sÃ£o sagrados
4. TESTADO     - Todo cÃ³digo tem teste automÃ¡tico
5. SUCINTO     - Sem documentaÃ§Ã£o excessiva
6. API-FIRST   - DigitalOcean API obrigatÃ³ria
7. CI/CD       - Deploy automÃ¡tico sempre
```

---

## ğŸš€ COMANDOS RÃPIDOS

```bash
# Deploy automÃ¡tico
python scripts/deploy_to_digitalocean.py

# Configurar CRON via API
python scripts/setup_cron_jobs.py

# Setup monitoramento
python scripts/setup_monitoring.py

# Rodar testes
pytest tests/ -v

# âŒ NUNCA fazer manualmente:
ssh user@161.35.128.174
crontab -e
nano arquivo.py
```

---

## ğŸ“ CHANGELOG

**v2.0.0 (08/02/2026) - ULTRA-STRICT**
- âœ… ProibiÃ§Ã£o absoluta de mudanÃ§as nÃ£o solicitadas
- âœ… AutomaÃ§Ã£o obrigatÃ³ria via DigitalOcean API
- âœ… Imutabilidade de base.py e scrapers existentes
- âœ… Testes automÃ¡ticos obrigatÃ³rios
- âœ… DocumentaÃ§Ã£o mÃ­nima
- âœ… CI/CD automÃ¡tico obrigatÃ³rio

---

**ESTE DOCUMENTO Ã‰ IMUTÃVEL**  
**VIOLAÃ‡Ã•ES SERÃƒO REJEITADAS**  
**AUTOMAÃ‡ÃƒO Ã‰ LEI**
